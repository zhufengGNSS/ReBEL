<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of demmlp1</title>
  <meta name="keywords" content="demmlp1">
  <meta name="description" content="DEMMLP1 Demonstrate simple regression using a multi-layer perceptron">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html &copy; 2003 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../menu.html">Home</a> &gt;  <a href="#">ReBEL-0.2.7</a> &gt; <a href="#">netlab</a> &gt; demmlp1.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../menu.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="menu.html">Index for .\ReBEL-0.2.7\netlab&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>demmlp1
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>DEMMLP1 Demonstrate simple regression using a multi-layer perceptron</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>This is a script file. </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment">DEMMLP1 Demonstrate simple regression using a multi-layer perceptron

    Description
    The problem consists of one input variable X and one target variable
    T with data generated by sampling X at equal intervals and then
    generating target data by computing SIN(2*PI*X) and adding Gaussian
    noise. A 2-layer network with linear outputs is trained by minimizing
    a  sum-of-squares error function using the scaled conjugate gradient
    optimizer.

    See also
    <a href="mlp.html" class="code" title="function net = mlp(nin, nhidden, nout, outfunc, prior, beta)">MLP</a>, <a href="mlperr.html" class="code" title="function [e, edata, eprior, mse] = mlperr(net, x, t)">MLPERR</a>, <a href="mlpgrad.html" class="code" title="function [g, gdata, gprior] = mlpgrad(net, x, t)">MLPGRAD</a>, <a href="scg.html" class="code" title="function [x, options, flog, pointlog, scalelog] = scg(f, x, options, gradf, varargin)">SCG</a></pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="mlp.html" class="code" title="function net = mlp(nin, nhidden, nout, outfunc, prior, beta)">mlp</a>	MLP	Create a 2-layer feedforward network.</li><li><a href="mlpfwd.html" class="code" title="function [y, z, a] = mlpfwd(net, x)">mlpfwd</a>	MLPFWD	Forward propagation through 2-layer network.</li><li><a href="netopt.html" class="code" title="function [net, options, varargout] = netopt(net, options, x, t, alg);">netopt</a>	NETOPT	Optimize the weights in a network model.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="demnlab.html" class="code" title="function demnlab(action);">demnlab</a>	DEMNLAB A front-end Graphical User Interface to the demos</li></ul>
<!-- crossreference -->


<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <span class="comment">%DEMMLP1 Demonstrate simple regression using a multi-layer perceptron</span>
0002 <span class="comment">%</span>
0003 <span class="comment">%    Description</span>
0004 <span class="comment">%    The problem consists of one input variable X and one target variable</span>
0005 <span class="comment">%    T with data generated by sampling X at equal intervals and then</span>
0006 <span class="comment">%    generating target data by computing SIN(2*PI*X) and adding Gaussian</span>
0007 <span class="comment">%    noise. A 2-layer network with linear outputs is trained by minimizing</span>
0008 <span class="comment">%    a  sum-of-squares error function using the scaled conjugate gradient</span>
0009 <span class="comment">%    optimizer.</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%    See also</span>
0012 <span class="comment">%    MLP, MLPERR, MLPGRAD, SCG</span>
0013 <span class="comment">%</span>
0014 
0015 <span class="comment">%    Copyright (c) Ian T Nabney (1996-2001)</span>
0016 
0017 
0018 <span class="comment">% Generate the matrix of inputs x and targets t.</span>
0019 
0020 ndata = 20;            <span class="comment">% Number of data points.</span>
0021 noise = 0.2;            <span class="comment">% Standard deviation of noise distribution.</span>
0022 x = [0:1/(ndata - 1):1]';
0023 randn(<span class="string">'state'</span>, 1);
0024 t = sin(2*pi*x) + noise*randn(ndata, 1);
0025 
0026 clc
0027 disp(<span class="string">'This demonstration illustrates the use of a Multi-Layer Perceptron'</span>)
0028 disp(<span class="string">'network for regression problems.  The data is generated from a noisy'</span>)
0029 disp(<span class="string">'sine function.'</span>)
0030 disp(<span class="string">' '</span>)
0031 disp(<span class="string">'Press any key to continue.'</span>)
0032 pause
0033 
0034 <span class="comment">% Set up network parameters.</span>
0035 nin = 1;            <span class="comment">% Number of inputs.</span>
0036 nhidden = 3;            <span class="comment">% Number of hidden units.</span>
0037 nout = 1;            <span class="comment">% Number of outputs.</span>
0038 alpha = 0.01;            <span class="comment">% Coefficient of weight-decay prior.</span>
0039 
0040 <span class="comment">% Create and initialize network weight vector.</span>
0041 
0042 net = <a href="mlp.html" class="code" title="function net = mlp(nin, nhidden, nout, outfunc, prior, beta)">mlp</a>(nin, nhidden, nout, <span class="string">'linear'</span>, alpha);
0043 
0044 <span class="comment">% Set up vector of options for the optimiser.</span>
0045 
0046 options = zeros(1,18);
0047 options(1) = 1;            <span class="comment">% This provides display of error values.</span>
0048 options(14) = 100;        <span class="comment">% Number of training cycles.</span>
0049 
0050 clc
0051 disp([<span class="string">'The network has '</span>, num2str(nhidden), <span class="string">' hidden units and a weight decay'</span>])
0052 disp([<span class="string">'coefficient of '</span>, num2str(alpha), <span class="string">'.'</span>])
0053 disp(<span class="string">' '</span>)
0054 disp(<span class="string">'After initializing the network, we train it use the scaled conjugate'</span>)
0055 disp(<span class="string">'gradients algorithm for 100 cycles.'</span>)
0056 disp(<span class="string">' '</span>)
0057 disp(<span class="string">'Press any key to continue'</span>)
0058 pause
0059 
0060 <span class="comment">% Train using scaled conjugate gradients.</span>
0061 [net, options] = <a href="netopt.html" class="code" title="function [net, options, varargout] = netopt(net, options, x, t, alg);">netopt</a>(net, options, x, t, <span class="string">'scg'</span>);
0062 
0063 disp(<span class="string">' '</span>)
0064 disp(<span class="string">'Now we plot the data, underlying function, and network outputs'</span>)
0065 disp(<span class="string">'on a single graph to compare the results.'</span>)
0066 disp(<span class="string">' '</span>)
0067 disp(<span class="string">'Press any key to continue.'</span>)
0068 pause
0069 
0070 <span class="comment">% Plot the data, the original function, and the trained network function.</span>
0071 plotvals = [0:0.01:1]';
0072 y = <a href="mlpfwd.html" class="code" title="function [y, z, a] = mlpfwd(net, x)">mlpfwd</a>(net, plotvals);
0073 fh1 = figure;
0074 plot(x, t, <span class="string">'ob'</span>)
0075 hold on
0076 xlabel(<span class="string">'Input'</span>)
0077 ylabel(<span class="string">'Target'</span>)
0078 axis([0 1 -1.5 1.5])
0079 [fx, fy] = fplot(<span class="string">'sin(2*pi*x)'</span>, [0 1]);
0080 plot(fx, fy, <span class="string">'-r'</span>, <span class="string">'LineWidth'</span>, 2)
0081 plot(plotvals, y, <span class="string">'-k'</span>, <span class="string">'LineWidth'</span>, 2)
0082 legend(<span class="string">'data'</span>, <span class="string">'function'</span>, <span class="string">'network'</span>);
0083 
0084 disp(<span class="string">' '</span>)
0085 disp(<span class="string">'Press any key to end.'</span>)
0086 pause
0087 close(fh1);
0088 clear all;</pre></div>
<hr><address>Generated on Tue 26-Sep-2006 10:36:21 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/">m2html</a></strong> &copy; 2003</address>
</body>
</html>